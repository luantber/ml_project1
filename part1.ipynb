{
 "cells": [
  {
   "source": [
    "<h1 align=\"center\">UNIVERSIDADE ESTADUAL DE CAMPINAS</h1> \n",
    "\n",
    "<h1 align=\"center\">INSTITUTE OF COMPUTING</h1> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2 align=\"center\">Machine Learninng</h2> \n",
    "<h2 align=\"center\">MC886A/MO444A</h2> \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Luis Bernal Chahuayo (RA 234923)\n",
    "- Jarol Butron Soria (RA 234833)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Introduction\n",
    "<p>Clustering allows us to be able to group a set of objects into subsets of objects called clusters, each cluster is made up of a collection of objects that are similar to each other, but that are different from the objects of other clusters. It can be used for the diagnosis of diseases through images, in the monitoring of social networks, marketing. Clustering is part of unsupervised learning, there are some clustering methods such as k-means, hierarchical clustering, K-medoids, DBSCAN, Hierarchical Clustering, etc.</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2. Clustering Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 K-Means\n",
    "\n",
    "The K-means algorithm is an iterative algorithm that attempts to divide the data set into K distinct non-overlapping predefined subgroups called clusters, where each data point belongs to a single group. Trying to make the intra-cluster data points as similar as possible while keeping the clusters as different as possible.\n",
    "\n",
    "- args:\n",
    "    - k : Number of clusters\n",
    "    - x : Dataset X-axis\n",
    "    - y : Dataset Y-axis\n",
    "\n",
    "- returns:\n",
    "    - punto_x: Centroide_x\n",
    "    - punto_y: Centroide_y\n",
    "    - matriz_clusters:\n",
    "    - distortion: The sum of all squared errors "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    args:\n",
    "        k : Number of clusters\n",
    "        x : Dataset X-axis\n",
    "        y : Dataset Y-axis\n",
    "        x_min: \n",
    "        x_max:\n",
    "        y_min: \n",
    "        y_max: \n",
    "\n",
    "    returns:\n",
    "        punto_x,\n",
    "        punto_y,\n",
    "        matriz_clusters,\n",
    "        distortion\n",
    "\"\"\"\n",
    "def kmeans(k,x,y):\n",
    "    \n",
    "    # Find min and max values    \n",
    "    x_min=np.min(x)\n",
    "    x_max=np.max(x)\n",
    "    y_min=np.min(y)\n",
    "    y_max=np.max(y)\n",
    "   \n",
    "    # Select K Random Points\n",
    "    punto_x=np.random.uniform(low=x_min,high=x_max,size=k)\n",
    "    punto_y=np.random.uniform(low=y_min,high=y_max,size=k)\n",
    "    \n",
    "    # Setting a huge distortion\n",
    "    # Used to stop algorithm\n",
    "    old_distortion = np.inf\n",
    "\n",
    "    for it in range(20):\n",
    "        # Create distance matrix\n",
    "        matriz_distancias = np.zeros((len(x),k))\n",
    "        for k_i in range(k):\n",
    "            # Euclidean distance\n",
    "            x_h=(x-punto_x[k_i])**2\n",
    "            y_h=(y-punto_y[k_i])**2\n",
    "            dist= np.sqrt(x_h+y_h)\n",
    "            matriz_distancias[:,k_i]=dist\n",
    "\n",
    "        # Find the minimum distance. Creation of mask matrix\n",
    "        min=np.argmin(matriz_distancias,axis=1)\n",
    "        matriz_clusters=np.zeros(matriz_distancias.shape)\n",
    "        i=0\n",
    "        for min_i in min:\n",
    "            matriz_clusters[i][min_i]=1\n",
    "            i+=1\n",
    "\n",
    "        # Column matrix transformation\n",
    "        x_r=x.reshape(-1,1)\n",
    "        y_r=y.reshape(-1,1)\n",
    "\n",
    "        # Sum of cluster values\n",
    "        matriz_clusters_x=x_r*matriz_clusters\n",
    "        matriz_clusters_y=y_r*matriz_clusters\n",
    "\n",
    "        total=matriz_clusters.sum(axis=0)\n",
    "\n",
    "        total_x=matriz_clusters_x.sum(axis=0)\n",
    "        total_y=matriz_clusters_y.sum(axis=0)\n",
    "\n",
    "        # Update point with mean total_x\n",
    "        punto_x=(total_x/ (total+0.0000000001) )\n",
    "        punto_y=(total_y/ (total+0.0000000001) )\n",
    "\n",
    "        # Cluster list\n",
    "        clusters = np.argmax(matriz_clusters,axis=1)\n",
    "        centroides_x = np.matmul(matriz_clusters,punto_x)\n",
    "        centroides_y = np.matmul(matriz_clusters,punto_y)\n",
    "        \n",
    "        distortion = Distortion(x,y,centroides_x,centroides_y)\n",
    "        \n",
    "        # Distortion - Condition\n",
    "        if ( abs( old_distortion - distortion ) <= 0.001 ): \n",
    "            break\n",
    "        else: old_distortion = distortion\n",
    "\n",
    "\n",
    "    print ( \"\\n \\n DISTORTION =\",distortion, \"Clusters=\", k)\n",
    "    plt.scatter(x,y,marker=\"o\",c=clusters)\n",
    "    plt.scatter(punto_x,punto_y,marker=\"*\",c=\"red\")\n",
    "    plt.show()\n",
    "\n",
    "    return punto_x,punto_y,matriz_clusters,distortion\n"
   ]
  },
  {
   "source": [
    "## 2.2 DBSCAN\n",
    "\n",
    "Explicar que hace el dbscan o como funciona\n",
    "\n",
    "\n",
    "\n",
    "- args:\n",
    "    - radio : Average distance \n",
    "    - x : Dataset X-axis\n",
    "    - y : Dataset Y-axi\n",
    "    - M : Minimum number of points\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    args:\n",
    "        k : Number of clusters\n",
    "        x : Dataset X-axis\n",
    "        y : Dataset Y-axis\n",
    "        x_min: \n",
    "        x_max:\n",
    "        y_min: \n",
    "        y_max: \n",
    "\n",
    "    returns:\n",
    "        punto_x,\n",
    "        punto_y,\n",
    "        matriz_clusters,\n",
    "        distortion\n",
    "\"\"\"\n",
    "def dbscan(x,y, radio, M ):\n",
    "    marcas = np.zeros( x.shape )\n",
    "   \n",
    "\n",
    "    # Repeat until all points are visited\n",
    "    while( True ):\n",
    "        \n",
    "        por_procesar = np.where( marcas == 0)\n",
    "        if ( len ( por_procesar[0] ) == 0): break\n",
    "\n",
    "        # Choose a point not visited \n",
    "        p_index = por_procesar[0][0] \n",
    "        punto_x = x[p_index]\n",
    "        punto_y = y[p_index]\n",
    "\n",
    "        # Get Distances \n",
    "        distancias = np.sqrt ( ( x - punto_x )**2 + ( y - punto_y )**2 )\n",
    "\n",
    "        # Select points that are neighboors from the choose point\n",
    "\n",
    "        n_vecinos = len ( distancias[ distancias < radio ]  )\n",
    "\n",
    "        # Classify \n",
    "        # 0 No worked \n",
    "        # 1 Core Point\n",
    "        # 2 Border\n",
    "        # 3 Outlier\n",
    "\n",
    "        if  n_vecinos >= M:\n",
    "            marcas[p_index] = 1\n",
    "        elif n_vecinos > 0 : \n",
    "            marcas[p_index] = 2\n",
    "        else: \n",
    "            marcas[p_index] = 3\n",
    "    \n",
    "    # -1 : No cluster\n",
    "    clusters = -1 * np.ones ( marcas.shape )\n",
    "\n",
    "    visitados = np.zeros ( marcas.shape  , dtype=bool)\n",
    "    \n",
    "    \n",
    "    core_points = np.where(marcas == 1)[0]\n",
    "    n_cluster = 0\n",
    "\n",
    "    # Repeat until all core points have been visited \n",
    "    while ( True ):\n",
    "        if ( len(core_points) == 0 ) : break\n",
    "        if  len(  np.where ( ( ~ visitados ) & ( clusters != -1  ) & ( marcas == 1 ) )[0]  ) == 0:\n",
    "            \n",
    "            #\n",
    "            c_p_x = x[core_points[0]]\n",
    "            c_p_y = y[core_points[0]]\n",
    "            visitados[ core_points[0] ] = 1 \n",
    "            distancias = np.sqrt ( ( x - c_p_x )**2 + ( y - c_p_y )**2 )\n",
    "            clusters[ np.where(distancias < radio)  ] = n_cluster \n",
    "            core_points = core_points[ 1 : ]\n",
    "            n_cluster += 1 \n",
    "        else:\n",
    "            # \n",
    "            core_points_nuevos = np.where ( ( ~ visitados ) & ( clusters != -1  ) & ( marcas == 1 ) )[0]      \n",
    "            c_p_x = x[core_points_nuevos[0]]\n",
    "            c_p_y = y[core_points_nuevos[0]]\n",
    "            visitados[ core_points_nuevos[0] ] = 1 \n",
    "            distancias = np.sqrt ( ( x - c_p_x )**2 + ( y - c_p_y )**2 )\n",
    "            clusters[ np.where(distancias < radio)  ] = clusters[ core_points_nuevos[0] ]\n",
    "            core_points = np.delete(core_points, np.where( core_points == core_points_nuevos[0] ))\n",
    "    \n",
    "    # print ( clusters )\n",
    "\n",
    "    # print(\"\\n \\nBorders and Core Points\")\n",
    "    # plt.scatter(x,y,c=marcas)\n",
    "    # plt.show()\n",
    "    \n",
    "    ## Getting centroides \n",
    "    matrix_clusters = pd.get_dummies(clusters).to_numpy()\n",
    "    \n",
    "    sumas = matrix_clusters.sum(axis=0)\n",
    "\n",
    "    x_matrix = x.reshape(-1,1)*matrix_clusters\n",
    "    y_matrix = y.reshape(-1,1)*matrix_clusters\n",
    "\n",
    "    x_centro = (x_matrix.sum(axis=0)/sumas)\n",
    "    \n",
    "    y_centro = (y_matrix.sum(axis=0)/sumas)\n",
    "    \n",
    "    # centroides_x = np.matmul(x_centro,matrix_clusters.T)\n",
    "    # centroides_y = np.matmul(y_centro,matrix_clusters.T)\n",
    "    \n",
    "    \n",
    "\n",
    "    # ( 1, 3 )  x ( 760  , 3  )\n",
    "    \n",
    "\n",
    "    print(\"Final Clusters formed\")\n",
    "    plt.scatter(x,y,c=clusters,s=3)\n",
    "    plt.show()\n",
    "\n",
    "    return x_centro,y_centro,matrix_clusters,matrix_clusters.shape[1]"
   ]
  },
  {
   "source": [
    "# 3. Evaluation Metrics "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.1 Distortion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "$$\n",
    "SSE=\\sum_{i=1}^{m} {(y_i - \\hat{y_i})}^{2}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Distortion(x,y,centroides_x,centroides_y):\n",
    "    distortion = ( ( x - centroides_x )**2 + ( y - centroides_y )**2 ).sum()\n",
    "    return distortion"
   ]
  },
  {
   "source": [
    "## 3.2 RMSSTD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Root mean squared error standard deviation (RMSSTD)\n",
    "\n",
    "$$\n",
    "RMSSTD= \\sqrt{\\frac{{\\sum_i}{\\sum_{x{\\in}c_i}}{{\\parallel x-c_i\\parallel}}^{2}}{p.{\\sum_i (n_i - 1)}}}\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSSTD(x,y,punto_x,punto_y,matriz_clusters,k):\n",
    "    centroides_x = np.matmul(matriz_clusters,punto_x)\n",
    "    centroides_y = np.matmul(matriz_clusters,punto_y)\n",
    "    distortion = Distortion(x,y,centroides_x,centroides_y)\n",
    "    number_attributes = 2 * ( len(x) - k )\n",
    "    return np.sqrt(distortion/number_attributes)"
   ]
  },
  {
   "source": [
    "# 4. Dataset 1: clusters.dat \n",
    "## 4.1 Load Datasets\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_table(\"cluster.dat\",header=None, sep=\"\\s+\")\n",
    "datos = pd.DataFrame(df)\n",
    "datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transforming the dataset to a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos=datos.to_numpy()\n",
    "datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Split Dataset (Train/test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(datos)\n",
    "datos"
   ]
  },
  {
   "source": [
    "- Split data in training/test sets (90/10) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_train = datos[:int(0.9*len(datos))]\n",
    "datos_test = datos[int(0.9*len(datos)):]\n",
    "\n",
    "x_train = datos_train[:,0]\n",
    "y_train = datos_train[:,1]\n",
    "\n",
    "x_test = datos_test[:,0]\n",
    "y_test = datos_test[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Pre-processing\n",
    "- Normalization (Min Max Scaler)\n",
    "$$\n",
    "    x_{scaled} = \\frac{( x - x_{min} ) }{ (x_{max}-x_{min}) }\n",
    "$$\n",
    "$$\n",
    "    y_{scaled} = \\frac{( y - y_{min} ) }{ (y_{max}-y_{min}) } \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x,y,x_max,x_min,y_max,y_min):\n",
    "    x=(x-x_min)/(x_max-x_min) \n",
    "    y=(y-y_min)/(y_max-y_min)\n",
    "    return x,y\n",
    "\n",
    "# It's important to save original parameters\n",
    "\n",
    "x_max_train = x_train.max()\n",
    "x_min_train= x_train.min()\n",
    "y_max_train = y_train.max()\n",
    "y_min_train = y_train.min()\n",
    "\n",
    "x_max_test = x_test.max()\n",
    "x_min_test = x_test.min()\n",
    "y_max_test = y_test.max()\n",
    "y_min_test = y_test.min()\n",
    "\n",
    "x_n_train, y_n_train = norm(x_train,y_train,x_max_train,x_min_train,y_max_train,y_min_train)\n",
    "x_n_test, y_n_test = norm(x_test,y_test,x_max_test,x_min_test,y_max_test,y_min_test)"
   ]
  },
  {
   "source": [
    "## 4.4 K-means Clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.4.1 Choosing the best K: Elbow Method\n",
    "- Evaluation of different numbers of clusters\n",
    "asdfasdf \n",
    "asdf asdf metodo "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(k_max, x_n_train , y_n_train ):\n",
    "    list_distortions = []\n",
    "    list_rmmstds = []\n",
    "    for k in range(1,k_max+1):\n",
    "        centroide_x , centroide_y , matriz_clusters , distortion = kmeans(k,x_n_train,y_n_train)\n",
    "        rmmstd = RMSSTD(x_n_train,y_n_train,centroide_x,centroide_y, matriz_clusters, k)\n",
    "        list_distortions.append(distortion)\n",
    "        list_rmmstds.append(rmmstd)\n",
    "    \n",
    "    print(\"\\n \\n Distortion\")\n",
    "    plt.plot(range(1,k_max+1),list_distortions,marker=\"*\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Root Mean Squared Standard Deviation\")\n",
    "    plt.plot(range(1,k_max+1),list_rmmstds,marker=\"o\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Execute with until 10 Clusters \n",
    "elbow_method(10,x_n_train,y_n_train)"
   ]
  },
  {
   "source": [
    "Observing the last graphic ( Elbow Method ), Disttortion and RMSSD.\n",
    "We choose **k=3**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3 \n",
    "centroide_x , centroide_y , matriz_clusters , distortion = kmeans(3,x_n_train,y_n_train)\n",
    "rmmstd = RMSSTD(x_n_train,y_n_train,centroide_x,centroide_y, matriz_clusters, 3)\n",
    "print( \"ERROR TRAIN:\", rmmstd )"
   ]
  },
  {
   "source": [
    "### Dataset Test "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(x,y,centroide_x,centroide_y,k):\n",
    "    matriz_distancias = np.zeros((len(x),k))\n",
    "    for k_i in range(k):\n",
    "        # Euclidean distance\n",
    "        x_h=(x-centroide_x[k_i])**2\n",
    "        y_h=(y-centroide_y[k_i])**2\n",
    "        dist= np.sqrt(x_h+y_h)\n",
    "        matriz_distancias[:,k_i]=dist\n",
    "\n",
    "    # Find the minimum distance. Creation of mask matrix\n",
    "    min=np.argmin(matriz_distancias,axis=1)\n",
    "    matriz_clusters=np.zeros(matriz_distancias.shape)\n",
    "    i=0\n",
    "    for min_i in min:\n",
    "        matriz_clusters[i][min_i]=1\n",
    "        i+=1\n",
    "    return matriz_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_clusters_test = get_clusters(x_n_test,y_n_test,centroide_x,centroide_y,3)\n",
    "rmmstd = RMSSTD(x_n_test,y_n_test,centroide_x,centroide_y, matriz_clusters_test , 3)\n",
    "print( \"ERROR TEST:\", rmmstd )"
   ]
  },
  {
   "source": [
    "## 4.5 DBSCAN\n",
    "It's not necessary to choose a K, becuase the algorithms finds it by himself. But other parameters must be chosen: Min_neighboors, radio_distance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
=======
   "source": [
    "centroide_x , centroide_y , matriz_clusters , k  = dbscan(x_n_train, y_n_train, 0.07 , 9 )\n",
    "rmmstd_train = RMSSTD(x_n_train,y_n_train,centroide_x,centroide_y, matriz_clusters, k)\n",
    "\n",
    "\n",
    "print( \"ERROR TRAIN:\", rmmstd_train )"
   ]
  },
  {
   "source": [
    "### Dataset Test "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03f28a6404fd834b01dfba9e29f53d9dff57c55
   "source": [
    "matriz_clusters_test = get_clusters(x_n_test,y_n_test,centroide_x,centroide_y,k)\n",
    "rmmstd = RMSSTD(x_n_test,y_n_test,centroide_x,centroide_y, matriz_clusters_test ,k)\n",
    "print( \"ERROR TEST:\", rmmstd )"
   ]
  },
  {
   "source": [
    "Comparing both RMSSTD's methods \n",
    "\n",
    "\n",
    "|       | Kmeans | DBSCAN |\n",
    "|-------|--------|--------|\n",
    "| Train | 0.1067 | 0.1064 |\n",
    "| Test  |   0.1110     | 0.1068       |\n",
    "\n",
    "We can conclude that DBSCAN, is lightly superior to kmeans in this scenario.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 5. Dataset 2: Vehicles Silohuetes\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.1 Description \n",
    "The dataset contains 3D objects within a 2D image by application of an ensemble of shape feature extractors to the 2D silhouettes of the objects. The features were extracted from the silhouettes by the HIPS (Hierarchical Image Processing System) extension BINATTS, which extracts a combination of scale independent features utilising both classical moments based measures such as scaled variance, skewness and kurtosis about the major/minor axes and heuristic measures such as hollows, circularity, rectangularity and compactness.\n",
    " - NUMBER OF EXAMPLES 946\n",
    " - NUMBER OF ATTRIBUTES 16\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.1  Attributes \n",
    "- COMPACTNESS\t(average perim)**2/area\n",
    "- CIRCULARITY\t(average radius)**2/area\n",
    "- DISTANCE CIRCULARITY\tarea/(av.distance from border)**2\n",
    "- RADIUS RATIO\t(max.rad-min.rad)/av.radius\n",
    "- PR.AXIS ASPECT RATIO\t(minor axis)/(major axis)\n",
    "- MAX.LENGTH ASPECT RATIO\t(length perp. max length)/(max length)\n",
    "- SCATTER RATIO\t(inertia about minor axis)/(inertia about major axis)\n",
    "- ELONGATEDNESS\t\tarea/(shrink width)**2\n",
    "- PR.AXIS RECTANGULARITY\tarea/(pr.axis length*pr.axis width)\n",
    "- MAX.LENGTH RECTANGULARITY area/(max.length*length perp. to this)\n",
    "- SCALED VARIANCE \t(2nd order moment about minor axis)/area \n",
    "ALONG MAJOR AXIS\n",
    "- SCALED VARIANCE \t(2nd order moment about major axis)/area\n",
    "\tALONG MINOR AXIS \n",
    "- SCALED RADIUS OF GYRATION\t(mavar+mivar)/area\n",
    "- SKEWNESS ABOUT \t(3rd order moment about major axis)/sigma_min**3\n",
    "\tMAJOR AXIS\n",
    "- SKEWNESS ABOUT \t(3rd order moment about minor axis)/sigma_maj**3\n",
    "\tMINOR AXIS\n",
    "- KURTOSIS ABOUT \t(4th order moment about major axis)/sigma_min**4\n",
    "\tMINOR AXIS  \n",
    "- KURTOSIS ABOUT \t(4th order moment about minor axis)/sigma_maj**4\n",
    "\tMAJOR AXIS\n",
    "- HOLLOWS RATIO\t(area of hollows)/(area of bounding polygon)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.1 Loading Dataset "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_table(\"merge-data\",delimiter=\" \",header=None)\n",
    "\n",
    "# Clean data \n",
    "del d[19] \n",
    "datos = pd.DataFrame(d).dropna()\n",
    "datos = datos.to_numpy()\n",
    "print(\"Filas, columnas\",datos.shape)\n",
    "\n",
    "colores = datos[ : , 18 ]\n",
    "datos = datos[ : , : 18 ]"
   ]
  },
  {
   "source": [
    "## 5.2 Pre-processing ( Dimensionality Reduction )"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "datos_pca = pca.fit_transform( datos  )"
   ]
  },
  {
   "source": [
    "## 5.2 Split Dataset Train/Test "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffe data\n",
    "np.random.shuffle(datos_pca)\n"
   ]
  },
  {
   "source": [
    "## 5.3 Pre-processing ( Normalization )\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_train = datos_pca[:int(0.9*len(datos))]\n",
    "datos_test = datos_pca[int(0.9*len(datos)):]\n",
    "\n",
    "x_train = datos_train[:,0]\n",
    "y_train = datos_train[:,1]\n",
    "\n",
    "x_test = datos_test[:,0]\n",
    "y_test = datos_test[:,1]\n",
    "\n",
    "x_max_train = x_train.max()\n",
    "x_min_train= x_train.min()\n",
    "y_max_train = y_train.max()\n",
    "y_min_train = y_train.min()\n",
    "\n",
    "x_max_test = x_test.max()\n",
    "x_min_test = x_test.min()\n",
    "y_max_test = y_test.max()\n",
    "y_min_test = y_test.min()\n",
    "\n",
    "x_n_train, y_n_train = norm(x_train,y_train,x_max_train,x_min_train,y_max_train,y_min_train)\n",
    "x_n_test, y_n_test = norm(x_test,y_test,x_max_test,x_min_test,y_max_test,y_min_test)"
   ]
  },
  {
   "source": [
    "## 5.4 KMEANS \n",
    "Choose the best k "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(10,x_n_train,y_n_train)"
   ]
  },
  {
   "source": [
    "By observation we could choose $k=3$ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
=======
   "source": [
    "# k = 3 \n",
    "centroide_x , centroide_y , matriz_clusters , distortion = kmeans(3,x_n_train,y_n_train)\n",
    "rmmstd = RMSSTD(x_n_train,y_n_train,centroide_x,centroide_y, matriz_clusters, 3)\n",
    "print( \"ERROR TRAIN:\", rmmstd )"
   ]
  },
  {
   "source": [
    "### Dataset Test "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03f28a6404fd834b01dfba9e29f53d9dff57c55
   "source": [
    "matriz_clusters_test = get_clusters(x_n_test,y_n_test,centroide_x,centroide_y,3)\n",
    "rmmstd = RMSSTD(x_n_test,y_n_test,centroide_x,centroide_y, matriz_clusters_test , 3)\n",
    "print( \"ERROR TEST:\", rmmstd )"
   ]
  },
  {
   "source": [
    "## 5.5 DBSCAN\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
=======
   "source": [
    "centroide_x , centroide_y , matriz_clusters , k  = dbscan(x_n_train, y_n_train, 0.026 , 6 )\n",
    "rmmstd_train = RMSSTD(x_n_train,y_n_train,centroide_x,centroide_y, matriz_clusters, k)\n",
    "\n",
    "\n",
    "print( \"ERROR TRAIN:\", rmmstd_train )"
   ]
  },
  {
   "source": [
    "### Dataset Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d03f28a6404fd834b01dfba9e29f53d9dff57c55
   "source": [
    "matriz_clusters_test = get_clusters(x_n_test,y_n_test,centroide_x,centroide_y,k)\n",
    "rmmstd = RMSSTD(x_n_test,y_n_test,centroide_x,centroide_y, matriz_clusters_test ,k)\n",
    "print( \"ERROR TEST:\", rmmstd )"
   ]
  },
  {
   "source": [
    "Comparing both RMSSTD's methods \n",
    "\n",
    "\n",
    "|       | Kmeans | DBSCAN |\n",
    "|-------|--------|--------|\n",
    "| Train | 0.0803 | 0.1031 |\n",
    "| Test  |   0.1316     | 0.1417       |\n",
    "\n",
    "In this case, Kmeans is superior. But it doesn't mean, the clusters are better. It could be intrepreted in several ways. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38564bit266af83202724f93b966d8ed714ce545",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}